{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "4TA7aZ9uSXYL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import copy\n",
        "import time\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "import pandas as pd\n",
        "experimental_data_log = []\n",
        "# YOUR LOGGING CODE - END\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA is available!\n",
            "Device name: NVIDIA GeForce RTX 4050 Laptop GPU\n",
            "CUDA version: 12.8\n"
          ]
        }
      ],
      "source": [
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"CUDA is available!\")\n",
        "    print(\"Device name:\", torch.cuda.get_device_name(0))  # Access the device name\n",
        "    print(\"CUDA version:\", torch.version.cuda) # Check CUDA version\n",
        "else:\n",
        "    print(\"CUDA is not available. You may need to install the CUDA toolkit and drivers.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "hTvlOBtMS3BR"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "imsize = 128 if not torch.cuda.is_available() else 256\n",
        "\n",
        "loader = transforms.Compose([\n",
        "    transforms.Resize((imsize, imsize)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "def image_loader(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    image = loader(image).unsqueeze(0)\n",
        "    return image.to(device, torch.float)\n",
        "\n",
        "def tensor_to_cv2(tensor):\n",
        "    image = tensor.cpu().clone().detach().squeeze(0)\n",
        "    image = transforms.ToPILImage()(image)\n",
        "    return cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "eFLflOs9S6-5"
      },
      "outputs": [],
      "source": [
        "class ContentLoss(nn.Module):\n",
        "    \"\"\"Computes the content loss between the target and input feature maps.\"\"\"\n",
        "    def __init__(self, target):\n",
        "        super().__init__()\n",
        "        # We 'detach' the target content from the tree used\n",
        "        # to dynamically compute the gradient: this is a stated value,\n",
        "        # not a variable. Otherwise the forward method of the criterion\n",
        "        # will throw an error.\n",
        "        self.target = target.detach()\n",
        "        self.loss = None # METRIC: To store loss value\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        self.loss = nn.functional.mse_loss(input_tensor, self.target)\n",
        "        return input_tensor\n",
        "\n",
        "def gram_matrix(input):\n",
        "    b, c, h, w = input.size()\n",
        "    features = input.view(b * c, h * w)\n",
        "    G = torch.mm(features, features.t())\n",
        "    return G.div(b * c * h * w)\n",
        "\n",
        "class StyleLoss(nn.Module):\n",
        "    def __init__(self, target_feature):\n",
        "        super().__init__()\n",
        "        self.target = gram_matrix(target_feature).detach()\n",
        "    def forward(self, x):\n",
        "        G = gram_matrix(x)\n",
        "        self.loss = nn.functional.mse_loss(G, self.target)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "4yeXUP_CTJyg"
      },
      "outputs": [],
      "source": [
        "cnn = models.vgg19(weights=models.VGG19_Weights.DEFAULT).features.to(device).eval()\n",
        "\n",
        "normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device)\n",
        "normalization_std = torch.tensor([0.229, 0.224, 0.225]).to(device)\n",
        "\n",
        "class Normalization(nn.Module):\n",
        "    def __init__(self, mean, std):\n",
        "        super().__init__()\n",
        "        self.mean = mean.view(-1, 1, 1)\n",
        "        self.std = std.view(-1, 1, 1)\n",
        "    def forward(self, img):\n",
        "        return (img - self.mean) / self.std\n",
        "\n",
        "def get_style_model_and_losses(cnn, normalization_mean, normalization_std,\n",
        "                                style_img, content_img,\n",
        "                                content_layers=['conv_4'],\n",
        "                                style_layers=['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']):\n",
        "    cnn = copy.deepcopy(cnn)\n",
        "\n",
        "    normalization = Normalization(normalization_mean, normalization_std).to(device)\n",
        "\n",
        "    content_losses = []\n",
        "    style_losses = []\n",
        "\n",
        "    model = nn.Sequential(normalization)\n",
        "\n",
        "    i = 0\n",
        "    for layer in cnn.children():\n",
        "        if isinstance(layer, nn.Conv2d):\n",
        "            i += 1\n",
        "            name = f'conv_{i}'\n",
        "        elif isinstance(layer, nn.ReLU):\n",
        "            name = f'relu_{i}'\n",
        "            layer = nn.ReLU(inplace=False)\n",
        "        elif isinstance(layer, nn.MaxPool2d):\n",
        "            name = f'pool_{i}'\n",
        "        elif isinstance(layer, nn.BatchNorm2d):\n",
        "            name = f'bn_{i}'\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        model.add_module(name, layer)\n",
        "\n",
        "        if name in content_layers:\n",
        "            target = model(content_img).detach()\n",
        "            content_loss = ContentLoss(target)\n",
        "            model.add_module(f\"content_loss_{i}\", content_loss)\n",
        "            content_losses.append(content_loss)\n",
        "\n",
        "        if name in style_layers:\n",
        "            target_feature = model(style_img).detach()\n",
        "            style_loss = StyleLoss(target_feature)\n",
        "            model.add_module(f\"style_loss_{i}\", style_loss)\n",
        "            style_losses.append(style_loss)\n",
        "\n",
        "    # Truncate the model after the last loss\n",
        "    for i in range(len(model) - 1, -1, -1):\n",
        "        if isinstance(model[i], (ContentLoss, StyleLoss)):\n",
        "            break\n",
        "    model = model[:i+1]\n",
        "\n",
        "    return model, style_losses, content_losses\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Style loss logging "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_style_transfer(cnn_model, norm_mean, norm_std,\n",
        "                       content_img_tensor, style_img_tensor, input_img_tensor,\n",
        "                       num_steps=200, style_weight=1e6, content_weight=1,\n",
        "                       log_every_k_steps=10): # New parameter for convergence logging frequency\n",
        "\n",
        "    # Build the style transfer model with loss layers\n",
        "    model, style_losses, content_losses = get_style_model_and_losses(\n",
        "        cnn_model, norm_mean, norm_std, style_img_tensor, content_img_tensor\n",
        "    ) \n",
        "    # We want to optimize the input image, not the model parameters\n",
        "    input_img_tensor.requires_grad_(True)\n",
        "    model.requires_grad_(False) # Model parameters are frozen\n",
        "\n",
        "    optimizer = optim.LBFGS([input_img_tensor]) \n",
        "    \n",
        "    convergence_this_run = [] # Log for step-wise convergence within this specific call\n",
        "    run = [0] # Current step counter, as a list to be modifiable in closure\n",
        "\n",
        "    while run[0] <= num_steps:\n",
        "        def closure():\n",
        "            # Correct the values of updated input image (clamp to [0,1])\n",
        "            with torch.no_grad():\n",
        "                input_img_tensor.clamp_(0, 1)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            model(input_img_tensor) # This populates sl.loss and cl.loss in style_losses and content_losses\n",
        "            \n",
        "            # Calculate raw scores (sum of losses from individual loss modules)\n",
        "            s_score_raw = sum(sl.loss for sl in style_losses)\n",
        "            c_score_raw = sum(cl.loss for cl in content_losses)\n",
        "\n",
        "            # Calculate the total loss for optimization\n",
        "            # Note: s_score_raw and c_score_raw are tensors here\n",
        "            loss = (s_score_raw * style_weight) + (c_score_raw * content_weight)\n",
        "            loss.backward()\n",
        "\n",
        "            run[0] += 1\n",
        "\n",
        "            # METRIC: Log convergence data at specified intervals or at the last step\n",
        "            if run[0] % log_every_k_steps == 0 or run[0] == num_steps:\n",
        "                convergence_this_run.append({\n",
        "                    'step': run[0],\n",
        "                    'total_loss': loss.item(),\n",
        "                    'weighted_style_loss': s_score_raw.item() * style_weight, # Log the weighted component\n",
        "                    'weighted_content_loss': c_score_raw.item() * content_weight # Log the weighted component\n",
        "                })\n",
        "            return loss\n",
        "        \n",
        "        optimizer.step(closure)\n",
        "\n",
        "    # Final clamp of the input image after optimization\n",
        "    with torch.no_grad():\n",
        "        input_img_tensor.clamp_(0, 1)\n",
        "    \n",
        "    # Calculate final weighted losses (these are the values often reported)\n",
        "    final_s_loss_val = sum(sl.loss.item() for sl in style_losses) * style_weight\n",
        "    final_c_loss_val = sum(cl.loss.item() for cl in content_losses) * content_weight\n",
        "    \n",
        "    return input_img_tensor.detach(), final_s_loss_val, final_c_loss_val, convergence_this_run # Return convergence log\n",
        "# === UPDATED run_style_transfer function END ==="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "style loss new one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_style_transfer(cnn_model, norm_mean, norm_std,\n",
        "                       content_img_tensor, style_img_tensor, input_img_tensor,\n",
        "                       num_steps=200, style_weight=1e6, content_weight=1):\n",
        "    \"\"\"Runs the style transfer algorithm.\"\"\"\n",
        "    # print('Building the style transfer model..')\n",
        "    model, style_losses, content_losses = get_style_model_and_losses(\n",
        "        cnn_model, norm_mean, norm_std, style_img_tensor, content_img_tensor)\n",
        "\n",
        "    # We want to optimize the input image, not the model parameters\n",
        "    input_img_tensor.requires_grad_(True)\n",
        "    model.requires_grad_(False) # Model parameters are frozen\n",
        "\n",
        "    optimizer = optim.LBFGS([input_img_tensor]) # LBFGS is a common optimizer for this\n",
        "\n",
        "    # print('Optimizing..')\n",
        "    run = [0]\n",
        "    while run[0] <= num_steps:\n",
        "        def closure():\n",
        "            # correct the values of updated input image\n",
        "            with torch.no_grad():\n",
        "                input_img_tensor.clamp_(0, 1)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            model(input_img_tensor)\n",
        "            \n",
        "            s_score = 0\n",
        "            c_score = 0\n",
        "\n",
        "            for sl in style_losses:\n",
        "                s_score += sl.loss\n",
        "            for cl in content_losses:\n",
        "                c_score += cl.loss\n",
        "\n",
        "            s_score *= style_weight\n",
        "            c_score *= content_weight\n",
        "\n",
        "            loss = s_score + c_score\n",
        "            loss.backward()\n",
        "\n",
        "            run[0] += 1\n",
        "            # METRIC: Optionally print loss at each step (can be verbose)\n",
        "            # if run[0] % 50 == 0:\n",
        "            #     print(f\"run {run[0]}:\")\n",
        "            #     print(f'Style Loss : {s_score.item():4f} Content Loss: {c_score.item():4f}')\n",
        "            #     print()\n",
        "            return loss\n",
        "\n",
        "        optimizer.step(closure)\n",
        "\n",
        "    # METRIC: Final clamp after optimization\n",
        "    with torch.no_grad():\n",
        "        input_img_tensor.clamp_(0, 1)\n",
        "    \n",
        "    # METRIC: Get final losses\n",
        "    final_style_loss = sum(sl.loss.item() for sl in style_losses) * style_weight\n",
        "    final_content_loss = sum(cl.loss.item() for cl in content_losses) * content_weight\n",
        "    \n",
        "    return input_img_tensor.detach(), final_style_loss, final_content_loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "style transfer main og"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Fz9_EyXKTcr0"
      },
      "outputs": [],
      "source": [
        "def run_style_transfer(cnn, norm_mean, norm_std, content_img, style_img, input_img, num_steps=200, style_weight=1e6, content_weight=1):\n",
        "    model, style_losses, content_losses = get_style_model_and_losses(\n",
        "        cnn, norm_mean, norm_std, style_img, content_img)\n",
        "\n",
        "    optimizer = optim.LBFGS([input_img.requires_grad_()])\n",
        "\n",
        "    run = [0]\n",
        "    while run[0] <= num_steps:\n",
        "\n",
        "        def closure():\n",
        "            optimizer.zero_grad()\n",
        "            model(input_img)\n",
        "            style_score = sum(sl.loss for sl in style_losses)\n",
        "            content_score = sum(cl.loss for cl in content_losses)\n",
        "\n",
        "            loss = style_score * style_weight + content_score * content_weight\n",
        "            loss.backward()\n",
        "\n",
        "            run[0] += 1\n",
        "            return loss\n",
        "\n",
        "        optimizer.step(closure)\n",
        "\n",
        "    return input_img.detach()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Execution alt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Capturing and stylizing webcam. Press 'q' to exit.\n"
          ]
        }
      ],
      "source": [
        "style_path = \"Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg\" # Make sure this image is in the same directory or provide the full path\n",
        "style_image = image_loader(style_path)\n",
        "\n",
        "cap = cv2.VideoCapture(0) # 0 for default webcam\n",
        "if not cap.isOpened():\n",
        "    print(\"Error: Could not open webcam.\")\n",
        "    exit()\n",
        "\n",
        "print(\"Capturing and stylizing webcam. Press 'q' to exit.\")\n",
        "\n",
        "# METRIC: FPS calculation variables\n",
        "frame_count = 0\n",
        "start_time = time.time()\n",
        "display_fps = 0\n",
        "\n",
        "# METRIC: Reduce num_steps for faster real-time processing, increase for better quality\n",
        "# For webcam, very few steps are feasible for real-time.\n",
        "# Even 10 steps can be slow depending on CPU/GPU and image size.\n",
        "# Start with a very low number like 5-10 for webcam, and increase if performance allows.\n",
        "# The original paper uses 300-500 steps for still images.\n",
        "webcam_num_steps = 200 # Significantly reduced for webcam\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        print(\"Error: Failed to capture frame from webcam.\")\n",
        "        break\n",
        "\n",
        "    # Resize frame to match `imsize` before converting to tensor\n",
        "    # This ensures consistency with how style_image is loaded if its original aspect ratio differs\n",
        "    resized_frame_for_tensor = cv2.resize(frame, (imsize, imsize))\n",
        "    \n",
        "    # Convert BGR frame (OpenCV default) to RGB for PIL and PyTorch\n",
        "    content_image_pil = Image.fromarray(cv2.cvtColor(resized_frame_for_tensor, cv2.COLOR_BGR2RGB))\n",
        "    content_image = loader(content_image_pil).unsqueeze(0).to(device)\n",
        "\n",
        "    # The input_image can be initialized from content_image or white noise\n",
        "    input_image = content_image.clone()\n",
        "    # Alternatively, for a different starting point:\n",
        "    # input_image = torch.randn(content_image.data.size(), device=device)\n",
        "\n",
        "\n",
        "    # METRIC: Record time for one frame's style transfer\n",
        "    loop_start_time = time.time()\n",
        "\n",
        "    output_tensor, final_s_loss, final_c_loss = run_style_transfer(\n",
        "        cnn, normalization_mean, normalization_std,\n",
        "        content_image, style_image, input_image,\n",
        "        num_steps=webcam_num_steps # Use reduced steps for webcam\n",
        "    )\n",
        "    \n",
        "    loop_time = time.time() - loop_start_time\n",
        "\n",
        "    styled_frame_cv2 = tensor_to_cv2(output_tensor)\n",
        "\n",
        "    # METRIC: Calculate overall FPS\n",
        "    frame_count += 1\n",
        "    elapsed_time = time.time() - start_time\n",
        "    if elapsed_time > 1: # Update FPS display every second\n",
        "        display_fps = frame_count / elapsed_time\n",
        "        frame_count = 0\n",
        "        start_time = time.time()\n",
        "\n",
        "    # METRIC: Display FPS and other metrics on the frame\n",
        "    cv2.putText(styled_frame_cv2, f\"FPS: {display_fps:.2f}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
        "    cv2.putText(styled_frame_cv2, f\"Style Loss: {final_s_loss:.2f}\", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
        "    cv2.putText(styled_frame_cv2, f\"Content Loss: {final_c_loss:.2f}\", (10, 80), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
        "    cv2.putText(styled_frame_cv2, f\"Loop Time: {loop_time:.2f}s\", (10, 100), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
        "    cv2.putText(styled_frame_cv2, f\"Steps: {webcam_num_steps}\", (10, 120), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
        "\n",
        "\n",
        "    # METRIC: PSNR and SSIM (require scikit-image and careful consideration of what to compare)\n",
        "    # Example: Compare original content *before* style transfer (resized) with the stylized output\n",
        "    # Ensure both images are in the same format (e.g., uint8, grayscale or RGB) and shape.\n",
        "    original_content_for_metric = cv2.cvtColor(resized_frame_for_tensor, cv2.COLOR_BGR2RGB) # Use the resized input\n",
        "    stylized_for_metric = cv2.cvtColor(styled_frame_cv2, cv2.COLOR_BGR2RGB) # Convert stylized output back to RGB\n",
        "    \n",
        "    if 'ssim' in globals() and 'psnr' in globals(): # Check if scikit-image functions were imported\n",
        "        try:\n",
        "            # Ensure images are uint8 for skimage metrics if they are not already\n",
        "            original_content_for_metric_uint8 = (original_content_for_metric).astype(np.uint8)\n",
        "            stylized_for_metric_uint8 = (styled_frame_cv2).astype(np.uint8) # styled_frame_cv2 is already BGR uint8\n",
        "            \n",
        "            # For SSIM, often better on grayscale, or specify multichannel=True for RGB\n",
        "            current_ssim = ssim(original_content_for_metric_uint8, stylized_for_metric_uint8, multichannel=True, data_range=255)\n",
        "            current_psnr = psnr(original_content_for_metric_uint8, stylized_for_metric_uint8, data_range=255)\n",
        "            \n",
        "            cv2.putText(styled_frame_cv2, f\"SSIM: {current_ssim:.2f}\", (10, 140), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
        "            cv2.putText(styled_frame_cv2, f\"PSNR: {current_psnr:.2f}\", (10, 160), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
        "        except Exception as e:\n",
        "            # print(f\"Metric calculation error: {e}\") # Can be noisy\n",
        "            pass\n",
        "\n",
        "\n",
        "    cv2.imshow(\"Neural Style Transfer - Webcam\", styled_frame_cv2)\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "exectuion orginal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IE9qJfRcTr4y",
        "outputId": "75386ba1-dc3d-46c3-b930-b7ded17b8190"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Capturing and stylizing webcam. Press 'q' to exit.\n"
          ]
        }
      ],
      "source": [
        "style_path = \"Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg\"\n",
        "style_image = image_loader(style_path)\n",
        "\n",
        "cap = cv2.VideoCapture(0)\n",
        "print(\"Capturing and stylizing webcam. Press 'q' to exit.\")\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    frame = cv2.resize(frame, (imsize, imsize))\n",
        "    content_image = loader(Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))).unsqueeze(0).to(device)\n",
        "\n",
        "    input_image = content_image.clone()\n",
        "\n",
        "    output = run_style_transfer(\n",
        "        cnn, normalization_mean, normalization_std,\n",
        "        content_image, style_image, input_image,\n",
        "        num_steps=200  # Reduce this for speed\n",
        "    )\n",
        "\n",
        "    styled_frame = tensor_to_cv2(output)\n",
        "\n",
        "    cv2.imshow(\"Neural Style Transfer\", styled_frame)\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "main loop logging part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Capturing and stylizing webcam. Press 'q' to exit.\n",
            "Current Configuration: webcam_steps200_imsize256_styleW1e+06_contentW1_style_Van_Gogh_-_Starry_Night_-_Google_Art_Project\n",
            "Experimental data saved to style_transfer_experiment2_log.csv\n"
          ]
        }
      ],
      "source": [
        "style_path = \"Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg\" \n",
        "style_image = image_loader(style_path)\n",
        "\n",
        "cap = cv2.VideoCapture(0)\n",
        "if not cap.isOpened():\n",
        "    print(\"Error: Could not open webcam.\")\n",
        "    exit()\n",
        "\n",
        "print(\"Capturing and stylizing webcam. Press 'q' to exit.\")\n",
        "\n",
        "frame_count = 0\n",
        "start_time = time.time()\n",
        "display_fps = 0.0 # Initialize as float\n",
        "\n",
        "# === CONFIGURATION FOR CURRENT EXPERIMENT RUN START ===\n",
        "# USER UPDATE: Modify these parameters for each experimental run\n",
        "webcam_num_steps = 200              \n",
        "# imsize is defined globally earlier (e.g., Cell In[3]), make sure it's what you want for this run.\n",
        "\n",
        "CONFIG_STYLE_WEIGHT = 1e6\n",
        "CONFIG_CONTENT_WEIGHT = 1\n",
        "# USER UPDATE: Ensure this variable is defined for your experiment.\n",
        "CONFIG_LOG_EVERY_K_STEPS = 5 # Example: Log convergence every 5 steps for webcam \n",
        "\n",
        "current_config_name = f\"webcam_steps{webcam_num_steps}_imsize{imsize}_styleW{CONFIG_STYLE_WEIGHT:.0e}_contentW{CONFIG_CONTENT_WEIGHT}_style_{style_path.split('/')[-1].split('.')[0]}\"\n",
        "print(f\"Current Configuration: {current_config_name}\")\n",
        "# === CONFIGURATION FOR CURRENT EXPERIMENT RUN END ===\n",
        "\n",
        "\n",
        "while True:\n",
        "    loop_start_time = time.time() \n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        print(\"Error: Failed to capture frame from webcam.\")\n",
        "        break\n",
        "\n",
        "    resized_frame_for_tensor = cv2.resize(frame, (imsize, imsize))\n",
        "    content_image_pil = Image.fromarray(cv2.cvtColor(resized_frame_for_tensor, cv2.COLOR_BGR2RGB))\n",
        "    content_image = loader(content_image_pil).unsqueeze(0).to(device)\n",
        "    input_image = content_image.clone()\n",
        "\n",
        "    # Unpack all four return values\n",
        "    output_tensor, final_s_loss, final_c_loss, convergence_log_for_frame = run_style_transfer(\n",
        "        cnn, normalization_mean, normalization_std,\n",
        "        content_image, style_image, input_image,\n",
        "        num_steps=webcam_num_steps,\n",
        "        style_weight=CONFIG_STYLE_WEIGHT,   \n",
        "        content_weight=CONFIG_CONTENT_WEIGHT,\n",
        "        log_every_k_steps=CONFIG_LOG_EVERY_K_STEPS # Pass the defined config variable\n",
        "    )\n",
        "    \n",
        "    current_loop_time = time.time() - loop_start_time\n",
        "\n",
        "    styled_frame_cv2 = tensor_to_cv2(output_tensor)\n",
        "\n",
        "    frame_count += 1\n",
        "    elapsed_time = time.time() - start_time\n",
        "    if elapsed_time >= 1.0: \n",
        "        current_display_fps = frame_count / elapsed_time\n",
        "        if current_display_fps > 0: \n",
        "            display_fps = current_display_fps \n",
        "        if current_display_fps > 0: \n",
        "             frame_count = 0\n",
        "             start_time = time.time()\n",
        "\n",
        "\n",
        "    cv2.putText(styled_frame_cv2, f\"FPS: {display_fps:.2f}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
        "    cv2.putText(styled_frame_cv2, f\"Style Loss: {final_s_loss:.2f}\", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
        "    cv2.putText(styled_frame_cv2, f\"Content Loss: {final_c_loss:.2f}\", (10, 80), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
        "    cv2.putText(styled_frame_cv2, f\"Loop Time: {current_loop_time:.2f}s\", (10, 100), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
        "    cv2.putText(styled_frame_cv2, f\"Steps: {webcam_num_steps}\", (10, 120), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
        "\n",
        "    # === LOGGING CODE - Log data for the current frame START ===\n",
        "    log_entry = {\n",
        "        'config_name': current_config_name,\n",
        "        'timestamp': time.time(), \n",
        "        'num_steps_config': webcam_num_steps,\n",
        "        'imsize_config': imsize,\n",
        "        'style_weight_config': CONFIG_STYLE_WEIGHT,\n",
        "        'content_weight_config': CONFIG_CONTENT_WEIGHT,\n",
        "        'style_image_path': style_path,\n",
        "        'loop_time_s': current_loop_time,\n",
        "        'fps_calculated': display_fps, \n",
        "        'final_weighted_style_loss': final_s_loss,\n",
        "        'final_weighted_content_loss': final_c_loss,\n",
        "        # USER UPDATE: Decide how to log 'convergence_log_for_frame'. \n",
        "        # For CSV, you might log its length or convert to JSON string if small.\n",
        "        'convergence_detail_steps': len(convergence_log_for_frame), \n",
        "        # 'convergence_log_json': pd.Series(convergence_log_for_frame).to_json(orient='records') # Example\n",
        "    }\n",
        "    experimental_data_log.append(log_entry)\n",
        "    # === LOGGING CODE - Log data for the current frame END ===\n",
        "\n",
        "    cv2.imshow(\"Neural Style Transfer - Webcam\", styled_frame_cv2)\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "# === LOGGING CODE - Save all logged data to CSV START ===\n",
        "output_csv_filename = 'style_transfer_experiment2_log.csv'\n",
        "df_results = pd.DataFrame(experimental_data_log)\n",
        "try:\n",
        "    df_results.to_csv(output_csv_filename, index=False)\n",
        "    print(f\"Experimental data saved to {output_csv_filename}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving experimental data to {output_csv_filename}: {e}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
