{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4TA7aZ9uSXYL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import copy\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA is not available. You may need to install the CUDA toolkit and drivers.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"CUDA is available!\")\n",
        "    print(\"Device name:\", torch.cuda.get_device_name(0))  # Access the device name\n",
        "    print(\"CUDA version:\", torch.version.cuda) # Check CUDA version\n",
        "else:\n",
        "    print(\"CUDA is not available. You may need to install the CUDA toolkit and drivers.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hTvlOBtMS3BR"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "imsize = 256 if not torch.cuda.is_available() else 512\n",
        "\n",
        "loader = transforms.Compose([\n",
        "    transforms.Resize((imsize, imsize)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "def image_loader(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    image = loader(image).unsqueeze(0)\n",
        "    return image.to(device, torch.float)\n",
        "\n",
        "def tensor_to_cv2(tensor):\n",
        "    image = tensor.cpu().clone().detach().squeeze(0)\n",
        "    image = transforms.ToPILImage()(image)\n",
        "    return cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "eFLflOs9S6-5"
      },
      "outputs": [],
      "source": [
        "class ContentLoss(nn.Module):\n",
        "    def __init__(self, target):\n",
        "        super().__init__()\n",
        "        self.target = target.detach()\n",
        "    def forward(self, x):\n",
        "        self.loss = nn.functional.mse_loss(x, self.target)\n",
        "        return x\n",
        "\n",
        "def gram_matrix(input):\n",
        "    b, c, h, w = input.size()\n",
        "    features = input.view(b * c, h * w)\n",
        "    G = torch.mm(features, features.t())\n",
        "    return G.div(b * c * h * w)\n",
        "\n",
        "class StyleLoss(nn.Module):\n",
        "    def __init__(self, target_feature):\n",
        "        super().__init__()\n",
        "        self.target = gram_matrix(target_feature).detach()\n",
        "    def forward(self, x):\n",
        "        G = gram_matrix(x)\n",
        "        self.loss = nn.functional.mse_loss(G, self.target)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4yeXUP_CTJyg"
      },
      "outputs": [],
      "source": [
        "cnn = models.vgg19(weights=models.VGG19_Weights.DEFAULT).features.to(device).eval()\n",
        "\n",
        "normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device)\n",
        "normalization_std = torch.tensor([0.229, 0.224, 0.225]).to(device)\n",
        "\n",
        "class Normalization(nn.Module):\n",
        "    def __init__(self, mean, std):\n",
        "        super().__init__()\n",
        "        self.mean = mean.view(-1, 1, 1)\n",
        "        self.std = std.view(-1, 1, 1)\n",
        "    def forward(self, img):\n",
        "        return (img - self.mean) / self.std\n",
        "\n",
        "def get_style_model_and_losses(cnn, normalization_mean, normalization_std,\n",
        "                                style_img, content_img,\n",
        "                                content_layers=['conv_4'],\n",
        "                                style_layers=['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']):\n",
        "    cnn = copy.deepcopy(cnn)\n",
        "\n",
        "    normalization = Normalization(normalization_mean, normalization_std).to(device)\n",
        "\n",
        "    content_losses = []\n",
        "    style_losses = []\n",
        "\n",
        "    model = nn.Sequential(normalization)\n",
        "\n",
        "    i = 0\n",
        "    for layer in cnn.children():\n",
        "        if isinstance(layer, nn.Conv2d):\n",
        "            i += 1\n",
        "            name = f'conv_{i}'\n",
        "        elif isinstance(layer, nn.ReLU):\n",
        "            name = f'relu_{i}'\n",
        "            layer = nn.ReLU(inplace=False)\n",
        "        elif isinstance(layer, nn.MaxPool2d):\n",
        "            name = f'pool_{i}'\n",
        "        elif isinstance(layer, nn.BatchNorm2d):\n",
        "            name = f'bn_{i}'\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        model.add_module(name, layer)\n",
        "\n",
        "        if name in content_layers:\n",
        "            target = model(content_img).detach()\n",
        "            content_loss = ContentLoss(target)\n",
        "            model.add_module(f\"content_loss_{i}\", content_loss)\n",
        "            content_losses.append(content_loss)\n",
        "\n",
        "        if name in style_layers:\n",
        "            target_feature = model(style_img).detach()\n",
        "            style_loss = StyleLoss(target_feature)\n",
        "            model.add_module(f\"style_loss_{i}\", style_loss)\n",
        "            style_losses.append(style_loss)\n",
        "\n",
        "    # Truncate the model after the last loss\n",
        "    for i in range(len(model) - 1, -1, -1):\n",
        "        if isinstance(model[i], (ContentLoss, StyleLoss)):\n",
        "            break\n",
        "    model = model[:i+1]\n",
        "\n",
        "    return model, style_losses, content_losses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fz9_EyXKTcr0"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "def run_style_transfer(cnn, norm_mean, norm_std, content_img, style_img, input_img, num_steps=200, style_weight=1e6, content_weight=1):\n",
        "    model, style_losses, content_losses = get_style_model_and_losses(\n",
        "        cnn, norm_mean, norm_std, style_img, content_img)\n",
        "\n",
        "    optimizer = optim.LBFGS([input_img.requires_grad_()])\n",
        "\n",
        "    run = [0]\n",
        "    while run[0] <= num_steps:\n",
        "\n",
        "        def closure():\n",
        "            optimizer.zero_grad()\n",
        "            model(input_img)\n",
        "            style_score = sum(sl.loss for sl in style_losses)\n",
        "            content_score = sum(cl.loss for cl in content_losses)\n",
        "\n",
        "            loss = style_score * style_weight + content_score * content_weight\n",
        "            loss.backward()\n",
        "\n",
        "            run[0] += 1\n",
        "            return loss\n",
        "\n",
        "        optimizer.step(closure)\n",
        "\n",
        "    return input_img.detach()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IE9qJfRcTr4y",
        "outputId": "75386ba1-dc3d-46c3-b930-b7ded17b8190"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Capturing and stylizing webcam. Press 'q' to exit.\n"
          ]
        }
      ],
      "source": [
        "style_path = \"C:/Users/anant/Downloads/Van_Gogh[1].jpg\"\n",
        "style_image = image_loader(style_path)\n",
        "\n",
        "cap = cv2.VideoCapture(0)\n",
        "print(\"Capturing and stylizing webcam. Press 'q' to exit.\")\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    frame = cv2.resize(frame, (imsize, imsize))\n",
        "    content_image = loader(Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))).unsqueeze(0).to(device)\n",
        "\n",
        "    input_image = content_image.clone()\n",
        "\n",
        "    output = run_style_transfer(\n",
        "        cnn, normalization_mean, normalization_std,\n",
        "        content_image, style_image, input_image,\n",
        "        num_steps=100  # Reduce this for speed\n",
        "    )\n",
        "\n",
        "    styled_frame = tensor_to_cv2(output)\n",
        "\n",
        "    cv2.imshow(\"Neural Style Transfer\", styled_frame)\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
